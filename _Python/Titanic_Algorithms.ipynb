{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbaseconda3006071fd14d4dc2ad1cee013cfc8c59",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in titanic training data\n",
    "titanic_df = pd.read_csv(\"../_Data/train_with_ages.csv\")\n",
    "titanic_testing_df = pd.read_csv(\"../_Data/test_with_ages.csv\")\n",
    "titanic_solutions_df = pd.read_csv(\"../_Data/solution_set.csv\")\n",
    "titanic_submission_df = titanic_testing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare solutions\n",
    "titanic_submission_df = titanic_submission_df.replace('\"', '', regex=True)\n",
    "titanic_solutions_df = titanic_solutions_df.replace('\"', '', regex=True)\n",
    "titanic_solutions_df = titanic_submission_df.merge(titanic_solutions_df, how='left', left_on=[\"Name\", \"Ticket\"], right_on=[\"name\",\"ticket\"])\n",
    "titanic_solutions_df = titanic_solutions_df[['PassengerId', 'survived']]\n",
    "#titanic_solutions_df.to_csv('../_Submission/010_temp_Submission_Solutions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the fare to '0' for titanic_testing_df index '152'\n",
    "def add_fare_values(fare):\n",
    "    if fare >= 0:\n",
    "        return fare\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "titanic_testing_df['Fare'] = titanic_testing_df.apply(lambda row: add_fare_values(row['Fare']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the per ticket price for each passenger.\n",
    "tickets = df.groupby(['Ticket'])['PassengerId'].count().reset_index()\n",
    "\n",
    "def fare_per_person(row):\n",
    "    fare_per_person = row['Fare']/tickets[tickets.Ticket==row['Ticket']]['PassengerId']\n",
    "    return fare_per_person.values[0]\n",
    "\n",
    "#df_processed['Fare_per_person'] = df_processed.apply(fare_per_person, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that data that will not likely be helpful\n",
    "titanic_df = titanic_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)\n",
    "titanic_testing_df = titanic_testing_df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Female to 0 and Male to 1\n",
    "titanic_df = titanic_df.replace('female', 0)\n",
    "titanic_df = titanic_df.replace('male', 1)\n",
    "\n",
    "titanic_testing_df = titanic_testing_df.replace('female', 0)\n",
    "titanic_testing_df = titanic_testing_df.replace('male', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe into numpy array\n",
    "titanic_np = titanic_df.to_numpy()\n",
    "titanic_testing_np = titanic_testing_df.to_numpy()\n",
    "print(f'Shape of titanic_np: {titanic_np.shape}')\n",
    "print(f'Shape of titanic_testing_np: {titanic_testing_np.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate numpy arrays for features and labels\n",
    "titanic_features = titanic_np[:, 1:]\n",
    "titanic_labels = titanic_np[:, 0:1]\n",
    "titanic_testing_features = titanic_testing_np\n",
    "\n",
    "print(f'Shape of titanic_features: {titanic_features.shape}')\n",
    "print(f'Shape of titanic_labels: {titanic_labels.shape}')\n",
    "print(f'Shape of titanic_testing_features: {titanic_testing_features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 90% training and 10% testing\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(titanic_features, titanic_labels, test_size=0.10, random_state=10)\n",
    "\n",
    "print(f'Shape of features_train: {features_train.shape}')\n",
    "print(f'Shape of labels_train: {labels_train.shape}')\n",
    "print(f'Shape of features_test: {features_test.shape}')\n",
    "print(f'Shape of labels_test: {labels_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a grid search for the chosen classifier\n",
    "def grid_search(classifier, param_grid):\n",
    "    classifier = GridSearchCV(classifier, param_grid, verbose=0, n_jobs=-1)\n",
    "    classifier.fit(features_train, labels_train)\n",
    "    classifier.best_params_\n",
    "    print(classifier.best_params_)\n",
    "    print(f'Best score: {classifier.best_score_}')\n",
    "\n",
    "# Train and validate the model for the chosen classifier\n",
    "def train_validate_model(model, algorithm):\n",
    "    model = model.fit(features_train, labels_train)\n",
    "    labels_predict = model.predict(features_test)\n",
    "\n",
    "    accuracy = accuracy_score(labels_predict, labels_test)\n",
    "    print(f\"Accuracy for {algorithm}: {accuracy:.3f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Predict Survived labels for testing dataset\n",
    "def predict_testing_labels(model):\n",
    "    labels_testing_predict = model.predict(titanic_testing_features)\n",
    "    return labels_testing_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Decision Tree Classifier\n",
    "algorithm = \"SVC\"\n",
    "\n",
    "if algorithm == \"DT\":\n",
    "    dtc_grid_search = tree.DecisionTreeClassifier()\n",
    "    param_grid = {'criterion': ['gini', 'entropy'], 'min_samples_split': [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30]}\n",
    "    grid_search(dtc_grid_search, param_grid)\n",
    "elif algorithm == \"LogReg\":\n",
    "    log_reg_grid_search = LogisticRegression()\n",
    "    param_grid = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 0.2, 0.4, 0.6, 0.8, 1, 1.2, 1.4, 1.6, 1.8, 2.0]}\n",
    "    grid_search(log_reg_grid_search, param_grid)\n",
    "elif algorithm == \"KNN\":\n",
    "    KNN_grid_search = KNeighborsClassifier()\n",
    "    param_grid = {'weights': ['uniform', 'distance'], 'n_neighbors': [1, 2, 6, 10, 12, 14, 16, 18, 20, 30, 40], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "    grid_search(KNN_grid_search, param_grid)\n",
    "elif algorithm == \"SVC\":\n",
    "    SVC_grid_search = SVC()\n",
    "    param_grid = {'kernel': ['linear', 'rbf', 'sigmoid'], 'C': [0.01, 0.2, 0.6, 1], 'gamma': ['scale', 'auto']}\n",
    "    grid_search(SVC_grid_search, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Classifier training and validation\n",
    "if algorithm == \"DT\":\n",
    "    dtc = tree.DecisionTreeClassifier(criterion='gini', min_samples_split=24)\n",
    "    model = train_validate_model(dtc, algorithm)\n",
    "    labels_testing_predict = predict_testing_labels(model)\n",
    "elif algorithm == \"LogReg\":\n",
    "    log_reg = LogisticRegression(penalty='l2', C=0.2)\n",
    "    model = train_validate_model(log_reg, algorithm)\n",
    "    labels_testing_predict = predict_testing_labels(model)\n",
    "elif algorithm == \"KNN\":\n",
    "    knn = KNeighborsClassifier(algorithm='brute', n_neighbors=30, weights='distance')\n",
    "    model = train_validate_model(knn, algorithm)\n",
    "    labels_testing_predict = predict_testing_labels(model)\n",
    "elif algorithm == \"SVC\":\n",
    "    svc = SVC(C=0.2, gamma='scale', kernel='linear')\n",
    "    model = train_validate_model(svc, algorithm)\n",
    "    labels_testing_predict = predict_testing_labels(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "if algorithm == \"ANN\":\n",
    "    # Convert the labels to one-hot encoding.\n",
    "    n_classes = 2\n",
    "    labels_train = np_utils.to_categorical(titanic_labels, n_classes)\n",
    "    print(f'Shape of one hot encoded labels_train: {labels_train.shape}')\n",
    "\n",
    "    # Add a 32 node hidden layer with sigmoid activation function.\n",
    "    n_features = titanic_features.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, input_dim=n_features, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, input_dim=n_features, activation='relu'))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, input_dim=n_features, activation='relu'))\n",
    "    model.add(Dense(n_classes, input_dim=n_features, activation='softmax'))\n",
    "\n",
    "    # Compile your model with accuracy as your metric.\n",
    "    opt=Adam(lr=0.001)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Train the model.\n",
    "    model_history = model.fit(titanic_features, labels_train, epochs=200, batch_size=128, validation_split=0.10)\n",
    "\n",
    "    # Predict the test set labels.\n",
    "    labels_testing_predict = model.predict(titanic_testing_features)\n",
    "    labels_testing_predict = np.argmax(labels_testing_predict, axis=-1)\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(model_history.history['accuracy'])\n",
    "    plt.plot(model_history.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy labels array to dataframe\n",
    "labels_testing_predict_df = pd.DataFrame(labels_testing_predict, index=None, columns=['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the predicted survival labels to the submission dataframe and change column to int64\n",
    "titanic_submission_df['Survived'] = labels_testing_predict_df['Survived']\n",
    "titanic_submission_df['Survived'] = titanic_submission_df['Survived'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns not needed for submission.\n",
    "titanic_submission_df_final = titanic_submission_df.drop(['Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to CSV results file\n",
    "#titanic_submission_df_final.to_csv('../_Submission/009_Ryan_Submission_ANN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy versus the Kaggle test solutions (doesn't affect previous csv export)\n",
    "titanic_submission_df_final = titanic_submission_df_final.drop(['PassengerId'], axis=1)\n",
    "titanic_solutions_df = titanic_solutions_df.drop(['PassengerId'], axis=1)\n",
    "titanic_solutions_df.columns = ['Survived']\n",
    "\n",
    "accuracy_test = accuracy_score(titanic_solutions_df, titanic_submission_df_final)\n",
    "print(f\"Accuracy for test dataset: {accuracy_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model results versus the Kaggle test set:\n",
    "\n",
    "Test set accuracy for Decision Tree: 0.749\n",
    "\n",
    "Test set accuracy for Logistic Regression: 0.763\n",
    "\n",
    "Test set accuracy for KNN: 0.639\n",
    "\n",
    "Test set accuracy for SVC: 0.766\n",
    "\n",
    "Test set accuracy for ANN: 0.744"
   ]
  }
 ]
}